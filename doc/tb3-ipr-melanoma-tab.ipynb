{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auteur: Antoine Cadiou\n",
    "\n",
    "# Attention: pour fonctionner il faut récupérer les données sur ces liens: \n",
    "### www.kaggle.com/dataset/5f79f0a1c3a7a203f986c8f7a9328af957ffef60b84bef4ef7216a39f5ca941a\n",
    "### https://www.kaggle.com/arroqc/siic-isic-224x224-images\n",
    "### www.kaggle.com/dataset/40f0f72b4735520283e9bb32bd11f40241ebb9f7aa6d236ee7736d561d01baaa\n",
    "#### et changer les chemins des données dans le code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, os\n",
    "import seaborn as sns\n",
    "from skimage import measure\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_curve, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, Dense, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "path = \"../input/tb3-ipr-melanoma-2020/data/PROJECT_Data/\"\n",
    "files = os.listdir(path)\n",
    "labels = pd.read_csv(\"../input/tb3-ipr-melanoma-2020/ISIC-2017_Data_GroundTruth_Classification.csv\")\n",
    "#labels.head()\n",
    "dataBIG = pickle.load(open(\"../input/isic224segmented/labels3.pickle\", \"rb\"))\n",
    "imgBIG = pickle.load(open(\"../input/isic224segmented/imgs3.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "names = []\n",
    "for f in files:\n",
    "    split = f.split('_')\n",
    "    if len(split)==2:\n",
    "        names.append(split[0]+\"_\"+split[1].split('.')[0])\n",
    "names=np.array(names)\n",
    "names.sort()\n",
    "names = pd.DataFrame(names)\n",
    "names.columns = ['image_id']\n",
    "\n",
    "data = names.join(labels, rsuffix=\"_bin_\")\n",
    "data['image_path']=path+data['image_id']+'.jpg'\n",
    "data['segmentation_path']=path+data['image_id']+'_segmentation.png'\n",
    "#data['superpixels_path']=data['image_id']+'_superpixels.png'\n",
    "data['label']=data['melanoma'].astype('int32')\n",
    "\n",
    "data = data.drop(['image_id', 'image_id_bin_', 'seborrheic_keratosis', 'melanoma'], axis=1)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldHorizontal(img, cx):\n",
    "    gauche = img[:,:cx]\n",
    "    droite = img[:,cx:]\n",
    "    l,cg = gauche.shape\n",
    "    l,cd = droite.shape\n",
    "    #on met les 2 folds aux mêmes dimensions en rajoutant du vide\n",
    "    if cg>cd:\n",
    "        droite = np.hstack((droite, np.zeros((l, cg-cd))))\n",
    "    else:\n",
    "        gauche = np.hstack((np.zeros((l, cd-cg)), gauche))\n",
    "    #on replie le gauche sur le droite\n",
    "    gauche_flip = cv2.flip(gauche, 1)\n",
    "    res = abs(droite-gauche_flip)\n",
    "    return np.sum(res)\n",
    "\n",
    "def foldVertical(img, cy):\n",
    "    haut = img[:cy,:]\n",
    "    bas = img[cy:,:]\n",
    "    lh,c = haut.shape\n",
    "    lb,c = bas.shape\n",
    "    #on met les 2 folds aux mêmes dimensions en rajoutant du vide\n",
    "    if lh>lb:\n",
    "        bas = np.vstack((bas, np.zeros((lh-lb, c))))\n",
    "    else:\n",
    "        haut = np.vstack((np.zeros((lb-lh, c)), haut))\n",
    "    #on replie le haut sur le bas\n",
    "    haut_flip = cv2.flip(haut, 0)\n",
    "    res = abs(haut_flip-bas)\n",
    "    return np.sum(res)\n",
    "    \n",
    "def getAsymmetry(img, cx, cy, A):\n",
    "    Ax = foldHorizontal(img, cx)\n",
    "    Ay = foldVertical(img, cy)\n",
    "    A1 = (min(Ax,Ay)/A)*100\n",
    "    A2 = (Ax + Ay)/A*100\n",
    "    return A1,A2\n",
    "\n",
    "def getBorderIrregularity(P, SD, GD):\n",
    "    return P * ((1/SD) - (1/GD))\n",
    "\n",
    "def getColorFeatures(imgcol, imgseg):\n",
    "    posL = np.argwhere(imgseg == 1)\n",
    "    Bl, Gl, Rl = np.mean(imgcol[posL[:,0],posL[:,1],:], axis=0)\n",
    "    posS = np.argwhere(imgseg == 0)\n",
    "    Bs, Gs, Rs = np.mean(imgcol[posS[:,0],posS[:,1],:], axis=0)\n",
    "\n",
    "    F1 = Rl/(Rl+Gl+Bl)\n",
    "    F2 = Gl/(Rl+Gl+Bl)\n",
    "    F3 = Bl/(Rl+Gl+Bl)\n",
    "    F4 = Rl/Rs\n",
    "    F5 = Gl/Gs\n",
    "    F6 = Bl/Bs\n",
    "    F7 = F4/(F4+F5+F6)\n",
    "    F8 = F5/(F4+F5+F6)\n",
    "    F9 = F6/(F4+F5+F6)\n",
    "    F10 = Rl-Rs\n",
    "    F11 = Gl-Gs\n",
    "    F12 = Bl-Bs\n",
    "    F13 = F10/(F10+F11+F12)\n",
    "    F14 = F11/(F10+F11+F12)\n",
    "    F15 = F12/(F10+F11+F12)\n",
    "    return [F4,F5,F6,F10,F11,F12,F13,F14,F15]\n",
    "    #return [F1,F2,F3,F4,F5,F6,F7,F8,F9,F10,F11,F12,F13,F14,F15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "N = len(data)\n",
    "\n",
    "for p in range(N):\n",
    "    psegment = data.segmentation_path[p]\n",
    "    pcolor = data.image_path[p]\n",
    "    # chargement des images\n",
    "    imgcol = cv2.imread(pcolor)\n",
    "    imgseg = cv2.imread(psegment)\n",
    "    imgseg = cv2.cvtColor(imgseg, cv2.COLOR_BGR2GRAY)/255.\n",
    "    label_imgseg = measure.label(imgseg)\n",
    "    props = measure.regionprops_table(label_imgseg, properties=['area', 'extent', 'perimeter', 'solidity', \\\n",
    "                                                                'major_axis_length', 'minor_axis_length', 'centroid'])\n",
    "    \n",
    "    #Region Properties\n",
    "    x = (np.array([props['extent'], \\\n",
    "                   props['solidity'], \\\n",
    "                   (props['minor_axis_length']/props['major_axis_length']),\\\n",
    "                   ((4*props['area'])/(np.pi * props['major_axis_length']**2)),\\\n",
    "                   ((np.pi*props['minor_axis_length'])/props['perimeter']),\\\n",
    "                   ((4*np.pi*props['area'])/props['perimeter']**2),\\\n",
    "                   (props['perimeter']/(np.pi * props['major_axis_length']))\n",
    "                  ]).T)[0]\n",
    "         \n",
    "    #Asymmetry\n",
    "    A1, A2 = getAsymmetry(imgseg, props['centroid-1'][0], props['centroid-0'][0], props['area'][0])\n",
    "    #Border Irregularity\n",
    "    B = getBorderIrregularity(props['perimeter'][0], props['minor_axis_length'][0], props['major_axis_length'][0])\n",
    "    #Color Features\n",
    "    CD = getColorFeatures(imgcol, imgseg)\n",
    "    \n",
    "    x = np.hstack((x, A1, A2, B, CD))\n",
    "    if len(X)==0:\n",
    "        X.append(x)\n",
    "    else:\n",
    "        X = np.vstack((X, x))\n",
    "    \n",
    "df = pd.DataFrame(X)\n",
    "df.columns = ['extent', 'solidity', 'd/D', '4A/(pi*d^2)', 'pi*d/P', '4*pi*A/P^2', 'P/(pi*D)','A1', 'A2', 'B'] +\\\n",
    "             ['F'+str(i) for i in range(1,len(CD)+1)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "i = 0\n",
    "while i<len(dataBIG):\n",
    "    imgcol = cv2.imread(\"../input/siic-isic-224x224-images/train/\"+dataBIG[\"filename\"][i])\n",
    "    imgseg = imgBIG[i]\n",
    "    imgseg = cv2.cvtColor(imgseg.astype('uint8'), cv2.COLOR_BGR2GRAY)/255.\n",
    "    #si on a une segmentation vide on la skip\n",
    "    if(np.all(imgseg==0)):\n",
    "        imgBIG = np.vstack((imgBIG[:i],imgBIG[i+1:]))\n",
    "        dataBIG = pd.concat([dataBIG.iloc[:i,:], dataBIG.iloc[i+1:,:]], ignore_index=True)\n",
    "        continue\n",
    "    \n",
    "    label_imgseg = measure.label(imgseg)\n",
    "    props = measure.regionprops_table(label_imgseg, properties=['area', 'extent', 'perimeter', 'solidity', \\\n",
    "                                                                'major_axis_length', 'minor_axis_length', 'centroid'])\n",
    "    \n",
    "    #Region Properties\n",
    "    x = (np.array([props['extent'], \\\n",
    "                   props['solidity'], \\\n",
    "                   (props['minor_axis_length']/props['major_axis_length']),\\\n",
    "                   ((4*props['area'])/(np.pi * props['major_axis_length']**2)),\\\n",
    "                   ((np.pi*props['minor_axis_length'])/props['perimeter']),\\\n",
    "                   ((4*np.pi*props['area'])/props['perimeter']**2),\\\n",
    "                   (props['perimeter']/(np.pi * props['major_axis_length']))\n",
    "                  ]).T)[0]\n",
    "         \n",
    "    #Asymmetry\n",
    "    A1, A2 = getAsymmetry(imgseg, props['centroid-1'][0], props['centroid-0'][0], props['area'][0])\n",
    "    #Border Irregularity\n",
    "    B = getBorderIrregularity(props['perimeter'][0], props['minor_axis_length'][0], props['major_axis_length'][0])\n",
    "    #Color Features\n",
    "    CD = getColorFeatures(imgcol, imgseg)\n",
    "    \n",
    "    x = np.hstack((x, A1, A2, B, CD))\n",
    "    if len(X)==0:\n",
    "        X.append(x)\n",
    "    else:\n",
    "        X = np.vstack((X, x))\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "dfBIG = pd.DataFrame(X)\n",
    "dfBIG.columns = ['extent', 'solidity', 'd/D', '4A/(pi*d^2)', 'pi*d/P', '4*pi*A/P^2', 'P/(pi*D)','A1', 'A2', 'B'] +\\\n",
    "             ['F'+str(i) for i in range(1,len(CD)+1)]\n",
    "dfBIG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dftrain = pd.DataFrame(scaler.fit_transform(dfBIG))\n",
    "Xtrain = np.array(dftrain)\n",
    "ytrain = np.array(dataBIG['target']=='malignant').astype('int64')\n",
    "ytrain_ohe = pd.get_dummies(dataBIG['target'])\n",
    "\n",
    "dftest = pd.DataFrame(scaler.fit_transform(df))\n",
    "Xtest = np.array(dftest)\n",
    "ytest = np.array(data['label'])\n",
    "ytest_ohe = pd.get_dummies(data['label'])\n",
    "\n",
    "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "clf2 = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', svm.SVC())\n",
    "])\n",
    "\n",
    "clf3 = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "clf4 = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "scores1 = cross_val_score(clf1, Xtrain, ytrain, cv=10, scoring=make_scorer(roc_auc_score))\n",
    "scores2 = cross_val_score(clf2, Xtrain, ytrain, cv=10, scoring=make_scorer(roc_auc_score))\n",
    "scores3 = cross_val_score(clf3, Xtrain, ytrain, cv=10, scoring=make_scorer(roc_auc_score))\n",
    "scores4 = cross_val_score(clf4, Xtrain, ytrain, cv=10, scoring=make_scorer(roc_auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = ['LogisticRegression', 'SVC', 'RandomForest', 'GradientBoosting']\n",
    "means = [scores1.mean(), scores2.mean(), scores3.mean(), scores4.mean()]\n",
    "stds = [scores1.std(), scores2.std(), scores3.std(), scores4.std()]\n",
    "x_pos = np.arange(len(clfs))\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(clfs)\n",
    "ax.set_title(\"Performances des modèles\")\n",
    "ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie où j'ai tenté des réseaux de neurones et un XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(input_size=(19)):\n",
    "#     inp = Input(input_size)\n",
    "#     d1 = Dense(15, activation='relu')(inp)\n",
    "#     d2 = Dense(10, activation='relu')(d1)\n",
    "#     d3 = Dense(7, activation='relu')(d2)\n",
    "#     d4 = Dense(5, activation='relu')(d3)\n",
    "#     outp = Dense(2, activation='softmax')(d4)\n",
    "#     model = Model(inp, outp)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\", tf.keras.metrics.AUC()]) #, tf.keras.metrics.AUC()\n",
    "#     return model\n",
    "\n",
    "# model = build_model()\n",
    "\n",
    "# h = model.fit(\n",
    "#     x = dftrain,\n",
    "#     y = ytrain_ohe, #ce sont les y 'one-hot-encoded'\n",
    "#     validation_split=0.2,\n",
    "#     batch_size=1,\n",
    "#     epochs=5,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# plt.plot(h.history['val_accuracy'], label='val_accuracy')\n",
    "# plt.plot(h.history['val_loss'], label='val_loss')\n",
    "# plt.plot(h.history[list(h.history.keys())[-1]], label='val_auc')\n",
    "# plt.legend()\n",
    "# plt.title(\"Visualisation des différents paramètres de performance de notre modèle\")\n",
    "# plt.show()\n",
    "\n",
    "# # h.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xgb.XGBClassifier(objective='binary:logistic', \\\n",
    "#                           max_depth=6, learning_rate=2.5, eval_metric='auc')\n",
    "# model.fit(Xtrain, ytrain)\n",
    "# ypred = model.predict(Xtest)\n",
    "# cm = confusion_matrix(ytest, (ypred>0.5).astype('int32'))\n",
    "# print(\"\\n\", cm)\n",
    "# print(\"\\n\", np.sum(np.diag(cm))/np.sum(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain_xgb = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "# Xtest_xgb = xgb.DMatrix(Xtest)\n",
    "\n",
    "# param = {'max_depth': 4, 'eta': 1.1, 'objective': 'binary:logistic'}\n",
    "# param['nthread'] = 4\n",
    "# param['eval_metric'] = 'auc'\n",
    "\n",
    "# evallist = [(Xtrain_xgb, 'train')]\n",
    "\n",
    "# num_round = 10\n",
    "# bst = xgb.train(param, Xtrain_xgb, num_round, evallist)\n",
    "# bst.save_model('xgb.txt')\n",
    "\n",
    "#bst = xgb.Booster({'nthread': 4})  # init model\n",
    "#bst.load_model('./xgb.txt')  # load data\n",
    "\n",
    "# ypred = bst.predict(Xtest_xgb)\n",
    "\n",
    "# cm = confusion_matrix(ytest, (ypred>0.5).astype('int32'))\n",
    "# print(\"\\n\", cm)\n",
    "# print(\"\\n\", np.sum(np.diag(cm))/np.sum(cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
